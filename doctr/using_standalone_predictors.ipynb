{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-75rJZasyjG"
      },
      "source": [
        "# Using docTR's standalone predictors\n",
        "\n",
        "docTR’s `ocr_predictor` acts as a modular wrapper for its individual prediction models.\n",
        "This notebook shows how to work with these models independently, which can be helpful if you don't need all the features of the `ocr_predictor`.\n",
        "For instance, you might want to pair docTR's `detection_predictor` with a different recognition model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "LH5a8koqsyjI"
      },
      "outputs": [],
      "source": [
        "# First we have to uninstall the preinstalled tensorflow version if we want to work with PyTorch as backend\n",
        "# because the env variables USE_TORCH=1 / USE_TF=1 doesn't have an effect in Colab\n",
        "!pip uninstall -y tensorflow\n",
        "# Install doctr\n",
        "#!pip install python-doctr[torch,viz]\n",
        "# From source\n",
        "!pip install python-doctr[torch,viz]@git+https://github.com/mindee/doctr.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RI0SCGXTsyjK"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import requests\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "\n",
        "from doctr.models import detection_predictor, recognition_predictor, page_orientation_predictor, crop_orientation_predictor\n",
        "from doctr.io import DocumentFile\n",
        "from doctr.utils.geometry import detach_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riI2cFwRsyjK"
      },
      "outputs": [],
      "source": [
        "# Define sample image urls\n",
        "\n",
        "# Image of receipt\n",
        "receipt = requests.get(\"https://github.com/mindee/doctr/releases/download/v0.3.0/mock_receipt.jpeg\").content\n",
        "receipt_image = cv2.imdecode(np.frombuffer(receipt, np.uint8), cv2.IMREAD_COLOR)\n",
        "\n",
        "# Image of a word crop\n",
        "word_crop = requests.get(\"https://github.com/mindee/doctr/releases/download/v0.5.1/word-crop.png\").content\n",
        "word_crop_image = cv2.imdecode(np.frombuffer(word_crop, np.uint8), cv2.IMREAD_COLOR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9mSG_bGsyjK"
      },
      "source": [
        "## Detection predictor\n",
        "\n",
        "The detection predictor can be used to detect text in an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvY10EFJsyjL"
      },
      "outputs": [],
      "source": [
        "# Helper function to convert relative coordinates to absolute pixel values\n",
        "def _to_absolute(geom, img_shape: tuple[int, int]) -> list[list[int]]:\n",
        "    h, w = img_shape\n",
        "    if len(geom) == 2:  # Assume straight pages = True -> [[xmin, ymin], [xmax, ymax]]\n",
        "        (xmin, ymin), (xmax, ymax) = geom\n",
        "        xmin, xmax = int(round(w * xmin)), int(round(w * xmax))\n",
        "        ymin, ymax = int(round(h * ymin)), int(round(h * ymax))\n",
        "        return [[xmin, ymin], [xmax, ymin], [xmax, ymax], [xmin, ymax]]\n",
        "    else:  # For polygons, convert each point to absolute coordinates\n",
        "        return [[int(point[0] * w), int(point[1] * h)] for point in geom]\n",
        "\n",
        "# Define the detection predictor\n",
        "det_predictor = detection_predictor(\n",
        "    arch=\"db_resnet50\",\n",
        "    pretrained=True,\n",
        "    assume_straight_pages=True,\n",
        "    symmetric_pad=True,\n",
        "    preserve_aspect_ratio=True,\n",
        "    batch_size=1,\n",
        ")  # .cuda().half()  # Uncomment this line if you have a GPU\n",
        "\n",
        "# Define the postprocessing parameters (optional)\n",
        "det_predictor.model.postprocessor.bin_thresh = 0.3\n",
        "det_predictor.model.postprocessor.box_thresh = 0.1\n",
        "\n",
        "# Load the document image\n",
        "docs = DocumentFile.from_images([receipt])\n",
        "results = det_predictor(docs)\n",
        "\n",
        "for doc, res in zip(docs, results):\n",
        "    img_shape = (doc.shape[0], doc.shape[1])\n",
        "    # Detach the probability scores from the results\n",
        "    detached_coords, prob_scores = detach_scores([res.get(\"words\")])\n",
        "\n",
        "    for i, coords in enumerate(detached_coords[0]):\n",
        "        coords = coords.reshape(2, 2).tolist() if coords.shape == (4,) else coords.tolist()\n",
        "\n",
        "        # Convert relative to absolute pixel coordinates\n",
        "        points = np.array(_to_absolute(coords, img_shape), dtype=np.int32).reshape((-1, 1, 2))\n",
        "\n",
        "        # Draw the bounding box on the image\n",
        "        cv2.polylines(receipt_image, [points], isClosed=True, color=(255, 0, 0), thickness=2)\n",
        "\n",
        "%matplotlib inline\n",
        "plt.imshow(cv2.cvtColor(receipt_image, cv2.COLOR_BGR2RGB)); plt.axis('off'); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXNJNmlosyjL"
      },
      "source": [
        "## Recognition predictor\n",
        "\n",
        "The recognition predictor is used to recognize text from pre-cropped word images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-uEr62MsyjL"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the word crop image\n",
        "doc = DocumentFile.from_images([word_crop])\n",
        "# Define the recognition predictor\n",
        "rec_predictor = recognition_predictor(arch=\"parseq\", pretrained=True, symmetric_pad=True, batch_size=1)  # .cuda().half()  # Uncomment this line if you have a GPU\n",
        "result = rec_predictor(doc)\n",
        "\n",
        "%matplotlib inline\n",
        "plt.imshow(cv2.cvtColor(word_crop_image, cv2.COLOR_BGR2RGB)); plt.axis('off'); plt.show()\n",
        "print(f\"Recognized text: {result[0][0]} \\nwith confidence: {result[0][1]:.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsXdh-NCsyjL"
      },
      "source": [
        "## Orientation predictors\n",
        "\n",
        "The orientation predictors can detect the **overall** orientation of a document image or word crop.\n",
        "They return the general orientation —[0, 90, 180, -90 (270)]— along with the corresponding confidence score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDL5ari7syjM"
      },
      "outputs": [],
      "source": [
        "\n",
        "docs = DocumentFile.from_images([receipt])\n",
        "page_orient_predictor = page_orientation_predictor(pretrained=True, batch_size=1)  # .cuda().half()  # Uncomment this line if you have a GPU\n",
        "result = page_orient_predictor(docs)\n",
        "print(f\"general crop orientation: {result[1][0]} with confidence: {result[2][0]:.2f}\")\n",
        "\n",
        "crop = DocumentFile.from_images([word_crop])\n",
        "crop_orient_predictor = crop_orientation_predictor(pretrained=True, batch_size=1)  # .cuda().half()  # Uncomment this line if you have a GPU\n",
        "result = crop_orient_predictor(crop)\n",
        "print(f\"general crop orientation: {result[1][0]} with confidence: {result[2][0]:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "doctr-torch-notebooks",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
